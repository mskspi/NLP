{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR_WQi4BRLMg"
      },
      "source": [
        "## Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt0hklma4Rh8",
        "outputId": "4fd7c6f2-3340-4276-d143-79c6c6ebc02f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.3.8\n",
            "  Downloading spacy-2.3.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.8) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.8) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.8) (3.0.8)\n",
            "Collecting thinc<7.5.0,>=7.4.1 (from spacy==2.3.8)\n",
            "  Downloading thinc-7.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.8) (0.7.9)\n",
            "Collecting wasabi<1.1.0,>=0.4.0 (from spacy==2.3.8)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting srsly<1.1.0,>=1.0.2 (from spacy==2.3.8)\n",
            "  Downloading srsly-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7 (from spacy==2.3.8)\n",
            "  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.8) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.8) (67.7.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.8) (1.22.4)\n",
            "Collecting plac<1.2.0,>=0.9.6 (from spacy==2.3.8)\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.8) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.8) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.8) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.8) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.8) (3.4)\n",
            "Installing collected packages: wasabi, plac, srsly, catalogue, thinc, spacy\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.6\n",
            "    Uninstalling srsly-2.4.6:\n",
            "      Successfully uninstalled srsly-2.4.6\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.9\n",
            "    Uninstalling thinc-8.1.9:\n",
            "      Successfully uninstalled thinc-8.1.9\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "confection 0.0.4 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 2.3.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed catalogue-1.0.2 plac-1.1.3 spacy-2.3.8 srsly-1.0.6 thinc-7.4.6 wasabi-0.10.1\n",
            "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_sm==2.3.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from en_core_web_sm==2.3.1) (2.3.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.8)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (67.7.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.22.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4)\n",
            "Building wheels for collected packages: en_core_web_sm\n",
            "  Building wheel for en_core_web_sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en_core_web_sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047087 sha256=0bdd79ab8a96c481e644c940c38d5d319034a2c0469cc4b283c25de34e8e0e4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/1f/0e/16fae4b01d2d87454e0f484e58c48793efcf237f0894c1c4bd\n",
            "Successfully built en_core_web_sm\n",
            "Installing collected packages: en_core_web_sm\n",
            "  Attempting uninstall: en_core_web_sm\n",
            "    Found existing installation: en-core-web-sm 3.5.0\n",
            "    Uninstalling en-core-web-sm-3.5.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.5.0\n",
            "Successfully installed en_core_web_sm-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: en_core_web_sm==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from en_core_web_sm==2.3.1) (2.3.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.8)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (67.7.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.22.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seaborn==0.11.1\n",
            "  Downloading seaborn-0.11.1-py3-none-any.whl (285 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.0/285.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.11.1) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.11.1) (1.10.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.11.1) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.11.1) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn==0.11.1) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn==0.11.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn==0.11.1) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn==0.11.1) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn==0.11.1) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn==0.11.1) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn==0.11.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn==0.11.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->seaborn==0.11.1) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn==0.11.1) (1.16.0)\n",
            "Installing collected packages: seaborn\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.12.2\n",
            "    Uninstalling seaborn-0.12.2:\n",
            "      Successfully uninstalled seaborn-0.12.2\n",
            "Successfully installed seaborn-0.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy==2.3.8\n",
        "!python -m spacy download en\n",
        "!pip install en_core_web_sm==2.3.1\n",
        "!pip install seaborn==0.11.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAM2PMLiKHin",
        "outputId": "ddf676f4-bae6-4938-efe7-9b9ae9409294"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu2fjc8GRR-Z"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DEh2O8AwKHis",
        "outputId": "17f40b9d-900e-4d99-add4-f0cdb3ca25c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dd76050b-b1c2-4f15-b41c-c8dd0f4d44e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PubMedID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4733</td>\n",
              "      <td>Huntington's chorea. Changes in neurotransmitt...</td>\n",
              "      <td>Neurotransmitter-receptor binding sites for ap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19702</td>\n",
              "      <td>Natural history of lactic acidosis after grand...</td>\n",
              "      <td>To define the time course of the metabolic aci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25385</td>\n",
              "      <td>A manpower policy for primary health care</td>\n",
              "      <td>A National Academy of Sciences study of policy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61558</td>\n",
              "      <td>Multiple sclerosis cerebrospinal fluid produce...</td>\n",
              "      <td>To investigate the myelinotoxicity of cerebrop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>61560</td>\n",
              "      <td>Need for alpha-fetoprotein assays</td>\n",
              "      <td>An alpha fetoprotein assay is useful in diagno...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd76050b-b1c2-4f15-b41c-c8dd0f4d44e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd76050b-b1c2-4f15-b41c-c8dd0f4d44e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd76050b-b1c2-4f15-b41c-c8dd0f4d44e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   PubMedID                                              Title  \\\n",
              "0      4733  Huntington's chorea. Changes in neurotransmitt...   \n",
              "1     19702  Natural history of lactic acidosis after grand...   \n",
              "2     25385          A manpower policy for primary health care   \n",
              "3     61558  Multiple sclerosis cerebrospinal fluid produce...   \n",
              "4     61560                  Need for alpha-fetoprotein assays   \n",
              "\n",
              "                                            Abstract  \n",
              "0  Neurotransmitter-receptor binding sites for ap...  \n",
              "1  To define the time course of the metabolic aci...  \n",
              "2  A National Academy of Sciences study of policy...  \n",
              "3  To investigate the myelinotoxicity of cerebrop...  \n",
              "4  An alpha fetoprotein assay is useful in diagno...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_p = pd.read_csv('DILIPositive.tsv', delimiter='\\t', header=0)\n",
        "df_p.head()\n",
        "\n",
        "df_n = pd.read_csv('DILINegative.tsv', delimiter='\\t', header=0)\n",
        "df_n.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCa2KALMNBOu"
      },
      "outputs": [],
      "source": [
        "# Combine title and abstract\n",
        "DATA=2     # 0: Title, 1: Abstract, 2: Title+Abstract\n",
        "if DATA==0:\n",
        "  df_p['Title_Abstract'] = df_p['Title'].fillna('') \n",
        "  df_n['Title_Abstract'] = df_n['Title'].fillna('')\n",
        "elif DATA==1:\n",
        "  df_p=df_p[~df_p['Abstract'].isnull()]\n",
        "  df_p['Title_Abstract'] = df_p['Abstract'].fillna('') \n",
        "\n",
        "  df_n=df_n[~df_n['Abstract'].isnull()]\n",
        "  df_n['Title_Abstract'] = df_n['Abstract'].fillna('')\n",
        "else: \n",
        "  df_p['Title_Abstract'] = df_p['Title'] + (' ' + df_p['Abstract']).fillna('')\n",
        "  df_n['Title_Abstract'] = df_n['Title'] + (' ' + df_n['Abstract']).fillna('')\n",
        "\n",
        "df_p['Class']='Positive'\n",
        "df_p_=df_p.iloc[:,3:5]\n",
        "df_p_.head()\n",
        "\n",
        "df_n['Class']='Negative'\n",
        "df_n_=df_n.iloc[:,3:5]\n",
        "df_n_.head()\n",
        "\n",
        "frames = [df_p_, df_n_]\n",
        "df=pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y00UDaiNKHi0"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
        "def cleanup_text(docs, logging=False):\n",
        "    texts = []\n",
        "    counter = 1\n",
        "    for doc in docs:\n",
        "        if counter % 1000 == 0 and logging:\n",
        "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
        "        counter += 1\n",
        "        doc = nlp(doc, disable=['parser', 'ner'])\n",
        "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
        "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
        "        tokens = ' '.join(tokens)\n",
        "        texts.append(tokens)\n",
        "    return pd.Series(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjxEwqISKHi5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC \n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import naive_bayes\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS \n",
        "from nltk.corpus import stopwords\n",
        "from spacy.lang.en import English\n",
        "\n",
        "spacy.load('en')\n",
        "parser = English()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geQLkL7-KHi6"
      },
      "outputs": [],
      "source": [
        "STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\n",
        "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kbZBQ3eKHi7"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "class CleanTextTransformer(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        return [cleanText(text) for text in X]\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "    \n",
        "def cleanText(text):\n",
        "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def tokenizeText(sample):\n",
        "    tokens = parser(sample)\n",
        "    lemmas = []\n",
        "    for tok in tokens:\n",
        "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
        "    tokens = lemmas\n",
        "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
        "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g92ZUDG9Rlqz"
      },
      "source": [
        "## Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiaNx0l2maJr"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class GensimWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None,\n",
        "                 sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5,\n",
        "                 ns_exponent=0.75, cbow_mean=1, hashfxn=hash, iter=5, null_word=0,\n",
        "                 trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,\n",
        "                 callbacks=(), max_final_vocab=None):\n",
        "        self.size = size\n",
        "        self.alpha = alpha\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.sample = sample\n",
        "        self.seed = seed\n",
        "        self.workers = workers\n",
        "        self.min_alpha = min_alpha\n",
        "        self.sg = sg\n",
        "        self.hs = hs\n",
        "        self.negative = negative\n",
        "        self.ns_exponent = ns_exponent\n",
        "        self.cbow_mean = cbow_mean\n",
        "        self.hashfxn = hashfxn\n",
        "        self.iter = iter\n",
        "        self.null_word = null_word\n",
        "        self.trim_rule = trim_rule\n",
        "        self.sorted_vocab = sorted_vocab\n",
        "        self.batch_words = batch_words\n",
        "        self.compute_loss = compute_loss\n",
        "        self.callbacks = callbacks\n",
        "        self.max_final_vocab = max_final_vocab\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.model_ = Word2Vec(\n",
        "            sentences=X, corpus_file=None,\n",
        "            vector_size=self.size, alpha=self.alpha, window=self.window, min_count=self.min_count,\n",
        "            max_vocab_size=self.max_vocab_size, sample=self.sample, seed=self.seed,\n",
        "            workers=self.workers, min_alpha=self.min_alpha, sg=self.sg, hs=self.hs,\n",
        "            negative=self.negative, ns_exponent=self.ns_exponent, cbow_mean=self.cbow_mean,\n",
        "            hashfxn=self.hashfxn, epochs=self.iter, null_word=self.null_word,\n",
        "            trim_rule=self.trim_rule, sorted_vocab=self.sorted_vocab, batch_words=self.batch_words,\n",
        "            compute_loss=self.compute_loss, callbacks=self.callbacks,\n",
        "            max_final_vocab=self.max_final_vocab)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_embeddings = np.array([self._get_embedding(words) for words in X])\n",
        "        return X_embeddings\n",
        "\n",
        "    def _get_embedding(self, words):\n",
        "        #valid_words = [word for word in words if word in self.model_.wv.vocab]\n",
        "        valid_words = [word for word in words if word in self.model_.wv.key_to_index ] # gensim >=4.0.0\n",
        "        #print(valid_words)\n",
        "        if valid_words:\n",
        "            embedding = np.zeros((len(valid_words), self.size), dtype=np.float32)\n",
        "            for idx, word in enumerate(valid_words):\n",
        "                embedding[idx] = self.model_.wv[word]\n",
        "\n",
        "            return np.mean(embedding, axis=0)\n",
        "        else:\n",
        "            return np.zeros(self.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDqyBPlIRXws"
      },
      "source": [
        "## Training and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z6jDwuqvKHi7",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Data split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import feature_selection\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy import sparse\n",
        "\n",
        "gensim_word2vec_tr = GensimWord2VecVectorizer(size=200, min_count=5,  sg=1, alpha=0.025, iter=10)\n",
        "                                              \n",
        "x_df = df['Title_Abstract'].tolist()\n",
        "y_df = df['Class'].tolist()\n",
        "\n",
        "# Tokenization\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenizeText, ngram_range=(1,1)) # TfidfVectorizer > CountVectorizer\n",
        "\n",
        "# Pipeline\n",
        "clf = LinearSVC(C=100)\n",
        "\n",
        "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
        "pipe_FS = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
        "pipe_FS_clf = Pipeline([('clf', clf)])\n",
        "pipe_w2v = Pipeline([('w2v', gensim_word2vec_tr)])\n",
        "\n",
        "REPEAT=30 \n",
        "acc_iter=[]\n",
        "f1_iter=[]\n",
        "recall_iter=[]\n",
        "precision_iter=[]\n",
        "\n",
        "for i in range(REPEAT):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.33, random_state=i, stratify=y_df)\n",
        "  \n",
        "  # TF-IDF\n",
        "  pipe_FS.fit(x_train)\n",
        "  x_train_FS=pipe_FS.transform(x_train)\n",
        "  x_test_FS=pipe_FS.transform(x_test)\n",
        "\n",
        "  # word2vec\n",
        "  clean_train=[]\n",
        "  for j in range(len(x_train)):\n",
        "    text=cleanText(x_train[j])\n",
        "    text1=tokenizeText(text)\n",
        "    clean_train.append(text1)\n",
        "\n",
        "  clean_test=[]\n",
        "  for j in range(len(x_test)):\n",
        "    text=cleanText(x_test[j])\n",
        "    text1=tokenizeText(text)\n",
        "    clean_test.append(text1)\n",
        "\n",
        "  pipe_w2v.fit(clean_train)\n",
        "  clean_train_w2v=pipe_w2v.transform(clean_train)\n",
        "  clean_test_w2v=pipe_w2v.transform(clean_test)\n",
        "\n",
        "  #combined\n",
        "  x_train_FS_dense=x_train_FS.todense()\n",
        "  x_test_FS_dense=x_test_FS.todense()\n",
        "  clean_train_w2v_m= np.asmatrix(clean_train_w2v)\n",
        "  clean_test_w2v_m= np.asmatrix(clean_test_w2v)\n",
        "\n",
        "  combined_train=np.concatenate((x_train_FS_dense, clean_train_w2v_m),axis=1)\n",
        "  combined_test=np.concatenate((x_test_FS_dense, clean_test_w2v_m),axis=1)\n",
        "\n",
        "  combined_train_sparse = sparse.csr_matrix(combined_train)\n",
        "  combined_test_sparse = sparse.csr_matrix(combined_test)\n",
        "  pipe_FS_clf.fit(combined_train_sparse, y_train)\n",
        "  preds = pipe_FS_clf.predict(combined_test_sparse)\n",
        "\n",
        "  print(\"accuracy:\", accuracy_score(y_test, preds))\n",
        "  acc_iter.append(accuracy_score(y_test, preds))\n",
        "  \n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, preds, labels=['Negative', 'Positive']).ravel()\n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  precision_iter.append(precision)\n",
        "  recall_iter.append(recall)\n",
        "  f1_iter.append(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgX4U9DsjCEn"
      },
      "outputs": [],
      "source": [
        "print(acc_iter)\n",
        "print(precision_iter)\n",
        "print(recall_iter)\n",
        "print(f1_iter)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}